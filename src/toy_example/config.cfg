[data]
# data_dir should contain train.src, train.trg, dev.src, dev.trg files
data_dir=/home/farajian/NLP/tools/NMT/nematus-15.11.2016/toy_example

# directory where output files will be generated
work_dir=/home/farajian/NLP/tools/NMT/nematus-15.11.2016/toy_example

# source corpus extension
src=en

# target corpus extension
trg=de

# number of bpe codes to learn from the source corpus
bpe_operation_src=40000

# number of bpe codes to learn from the target corpus
bpe_operation_trg=40000

# Subword pre-processing to be performed
apply_bpe=False

[nmt-architecture]
# source vocabulary size
n_words_src=4167

# target vocabulary size
n_words=5121

# maximum length of the sentence
maxlen=50

# word vector dimensionality
dim_word=100

# the number of LSTM units
dim=200

factors=1

dim_per_factor=[100]

batch_size=100

valid_batch_size=100

# reload model parameter if a model already exist
reload_=True

# overwrite previous saved model
overwrite=False

# optimizer algorithm to use
optimizer=adagrad

# learning rate
lrate=0.01

# 
dispFreq=1

# the development set will be evaluated after this many updates
validFreq=100000

# save the model parameters after every saveFreq updates
saveFreq=10

# generate some samples after every sampleFreq (do not reduce this value, it may cause some trouble)
sampleFreq=1000000

use_dropout=False

# dropout for input embeddings (0: no dropout)
dropout_embedding=0.2

# dropout for hidden layers (0: no dropout)
dropout_hidden=0.2

# dropout source words (0: no dropout)
dropout_source=0.1

# dropout target words (0: no dropout)
dropout_target=0.1

# should the parallel corpus be shuffled for each new epoch
shuffle_each_epoch=False

# after this many epoxh the program will terminate
max_epochs=100

# finish after this many updates
finish_after=10000000

finetune=False

finetune_only_last=False

sort_by_length=False

use_domain_interpolation=False

domain_interpolation_min=0.1

domain_interpolation_inc=0.1

domain_interpolation_indomain_datasets=['indomain.en', 'indomain.fr'] # don't change since use_domain_interpolation=0

maxibatch_size=20

# L2 regularization penalty
decay_c=0.0

# L2 regularization penalty towards original weights
map_decay_c=0.

# alignment regularization
alpha_c=0.0

# gradient clipping threshold
clip_c=1.0

# early stopping patience
patience=1000

encoder=gru

decoder=gru_cond
